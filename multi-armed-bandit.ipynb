{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Armed Bandit\n",
    "=============\n",
    "- 손잡이가 n개인 슬롯머신\n",
    "- 시간의 흐름에 따라 높은 보상을 주는 손잡이를 찾고\n",
    "- 그 손잡이에서 오는 보상을 최대화 하는 문제\n",
    "\n",
    "## 강화학습의 3가지 요소\n",
    "1. 액션 의존성 - 각 액션은 다른 보상을 가져온다\n",
    "2. 시간 의존성 - 보상은 시간이 지난 뒤에 주어지고, 드물게(sparse) 주어진다\n",
    "3. 상태 의존성 - 보상은 환경의 상태에 좌우된다\n",
    "\n",
    "#### 정책 (Policy)\n",
    "- 한 액션이 어떤 보상과 관련이 있는지,\n",
    "- 그래서 에이전트가 최적의 액션을 선택하도록 하는 것\n",
    "- 시간이 지난 후 최대의 보상을 얻는 정책이 바로 최적의 정책\n",
    "\n",
    "## 정책 경사\n",
    "- NN은 일련의 가중치로 구성되는데,\n",
    "- MAB문제는 이를 각각의 손잡이에 매치되어 결정된다\n",
    "- 가중치를 초기화 할 때 모두 1로 하면 모든 보상에 대해 낙관적으로 시작한다는 의미가 된다\n",
    "\n",
    "#### 예) Greedy Policy\n",
    "- 네트워크 업데이트 시 단순히 e의 확률로 랜덤 액션을 취하는 policy(손잡이)를 정했다고 가정\n",
    "- 에이전트는 기본적으로 최댓값을 예상하는 액션을 선택\n",
    "- 하지만 랜덤(e)의 확률로 다른 액션을 취함\n",
    "- 이렇게 여러 시도를 해보면서 학습을 진행하게 되고, 보상은 1 또는 -1로 받게된다\n",
    "- 이 비용을 수식(정책 비용 수식, Policy Loss Equation)으로 표현하면: \n",
    "\n",
    "                                   Loss = -log(pi) * A\n",
    "\n",
    "#### 수식 설명\n",
    "- A는 어드밴티지(Advantage) - 현재 취하는 액션이 미리 정한 기준선보다 얼마나 나은지의 정도\n",
    "    - 이는 결국 에이전트가 각 액션에 대해 받는 보상\n",
    "- pi는 정책(Policy) - 선택한 액션의 가중치\n",
    "- 비용 함수 (Loss Function) - (+)의 보상에 대해 가중치 증가, (-)의 보상에 대해 가중치 감소의 목적\n",
    "\n",
    "## 코드예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 밴딧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손잡이가 4개인 밴딧\n",
    "bandit_arms = [0.2, 0, -0.2, -2]\n",
    "num_arms = len(bandit_arms)\n",
    "\n",
    "#랜덤한 정규분포를 생성하고, bandit보다 크면 1 아니면 -1을 반환\n",
    "def pullBandit(bandit):\n",
    "    #랜덤한 값을 구함\n",
    "    result = np.random.randn(1)\n",
    "    \n",
    "    if result > bandit: #(+)보상 반환\n",
    "        return 1 \n",
    "    else: #(-)보상 반환\n",
    "        return -1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#NN의 feed-forward를 구현\n",
    "#값을 1로 시작 (1이면 모든 보상에 대해 낙관적으로 시작한다는 의미)\n",
    "weights = tf.Variable(tf.ones([num_arms]))\n",
    "output = tf.nn.softmax(weights)\n",
    "\n",
    "# 학습 과정을 구현\n",
    "# 보상과 선택된 액션을 NN에 feed --> 비용을 계산\n",
    "# 비용을 사용해 NN을 업데이트\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output, action_holder, [1])\n",
    "# 위 예제에서 설정한 Loss함수 (Loss = -log(pi) * A)\n",
    "loss = -(tf.log(responsible_output) * reward_holder)\n",
    "# Adam을 사용하고 학습률을 0.001로 설정\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에이전트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the  4 arms of the bandit:  [1. 0. 0. 0.]\n",
      "Running reward for the  4 arms of the bandit:  [-2.  2.  2.  5.]\n",
      "Running reward for the  4 arms of the bandit:  [-5.  1.  1. 12.]\n",
      "Running reward for the  4 arms of the bandit:  [-7. -3.  0. 17.]\n",
      "Running reward for the  4 arms of the bandit:  [-5. -8.  2. 20.]\n",
      "Running reward for the  4 arms of the bandit:  [-5. -7. -1. 30.]\n",
      "Running reward for the  4 arms of the bandit:  [-9. -4.  1. 37.]\n",
      "Running reward for the  4 arms of the bandit:  [-9. -5.  0. 45.]\n",
      "Running reward for the  4 arms of the bandit:  [-7. -5.  5. 54.]\n",
      "Running reward for the  4 arms of the bandit:  [-13.  -6.   6.  58.]\n",
      "Running reward for the  4 arms of the bandit:  [-14.  -7.  10.  64.]\n",
      "Running reward for the  4 arms of the bandit:  [-18.  -8.  17.  72.]\n",
      "Running reward for the  4 arms of the bandit:  [-21.  -9.  20.  79.]\n",
      "Running reward for the  4 arms of the bandit:  [-21.  -8.  19.  87.]\n",
      "Running reward for the  4 arms of the bandit:  [-22.  -8.  19.  94.]\n",
      "Running reward for the  4 arms of the bandit:  [-26. -13.  20.  98.]\n",
      "Running reward for the  4 arms of the bandit:  [-24. -16.  18. 105.]\n",
      "Running reward for the  4 arms of the bandit:  [-28. -19.  22. 116.]\n",
      "Running reward for the  4 arms of the bandit:  [-29. -20.  23. 123.]\n",
      "Running reward for the  4 arms of the bandit:  [-29. -23.  31. 132.]\n",
      "Running reward for the  4 arms of the bandit:  [-29. -24.  30. 140.]\n",
      "Running reward for the  4 arms of the bandit:  [-36. -23.  31. 151.]\n",
      "Running reward for the  4 arms of the bandit:  [-36. -28.  31. 158.]\n",
      "Running reward for the  4 arms of the bandit:  [-36. -25.  35. 167.]\n",
      "Running reward for the  4 arms of the bandit:  [-35. -25.  32. 177.]\n",
      "Running reward for the  4 arms of the bandit:  [-37. -28.  36. 184.]\n",
      "Running reward for the  4 arms of the bandit:  [-41. -26.  39. 189.]\n",
      "Running reward for the  4 arms of the bandit:  [-44. -28.  42. 197.]\n",
      "Running reward for the  4 arms of the bandit:  [-49. -31.  46. 205.]\n",
      "Running reward for the  4 arms of the bandit:  [-49. -31.  48. 215.]\n",
      "Running reward for the  4 arms of the bandit:  [-52. -32.  51. 222.]\n",
      "Running reward for the  4 arms of the bandit:  [-52. -31.  54. 228.]\n",
      "Running reward for the  4 arms of the bandit:  [-55. -33.  56. 239.]\n",
      "Running reward for the  4 arms of the bandit:  [-53. -30.  54. 248.]\n",
      "The agent thinks arm  4  is the most promissing....\n",
      "... and it was right!\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# 학습시킬 에피소드의 수\n",
    "total_episodes = 1000\n",
    "# 각 밴딧에 대한 점수판, 0으로 시작\n",
    "total_reward = np.zeros(num_arms)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#텐서플로 그래프를 론칭\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        #볼츠만 분포에 따라 액션 선택\n",
    "        actions = sess.run(output)\n",
    "        a = np.random.choice(actions, p=actions)\n",
    "        action = np.argmax(actions == a)\n",
    "        \n",
    "        #밴딧 중 하나를 선택해 보상을 취함\n",
    "        reward = pullBandit(bandit_arms[action])\n",
    "        \n",
    "        # NN의 weight(가중치)를 업데이트\n",
    "        _, resp, ww = sess.run([update, responsible_output, weights], \n",
    "                               feed_dict={reward_holder: [reward], action_holder: [action]})\n",
    "        \n",
    "        # 보상의 합계 업데이트\n",
    "        total_reward[action] += reward\n",
    "        \n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            print(\"Running reward for the \", str(num_arms), \"arms of the bandit: \", str(total_reward))\n",
    "        i += 1\n",
    "\n",
    "print(\"The agent thinks arm \", str(np.argmax(ww) + 1), \" is the most promissing....\")\n",
    "\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "    print(\"... and it was right!\")\n",
    "else:\n",
    "    print(\"... and it was wrong!\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과\n",
    "처음에 지정했듯, (+)리워드를 가장 많이 받을 수 있는 -2밴딧이 가장 높게 나왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
